{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aafaf899-221a-40db-b06a-dc02408d9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ali shabbir\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25061e3d-7ac5-4263-a1c3-f2aaef37d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b55d7a18-ac50-45ca-b97f-ddfff6236502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the webpage!\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/'  # Replace with your target website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the webpage!\")\n",
    "else:\n",
    "    print(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3b9c0c-d544-4343-8311-23c75d12202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f56ed16a-faaf-4660-a50c-9943b0b19cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: Web Scraping Python Tutorial – How to Scrape Data From A Website\n"
     ]
    }
   ],
   "source": [
    "title = soup.title.text\n",
    "print(f\"Page Title: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94cc6efb-8e4a-41af-9dc0-8177ef9d6e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python is a beautiful language to code in. It has a great package ecosystem, there's much less noise than you'll find in other languages, and it is super easy to use.\n",
      "Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping. \n",
      "In this article, we will cover how to use Python for web scraping. We'll also work through a complete hands-on classroom guide as we proceed.\n",
      "Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.\n",
      "If you want to code along, you can use this free codedamn classroom that consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.\n",
      "In this classroom, you'll be using this page to test web scraping: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\n",
      "This classroom consists of 7 labs, and you'll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.\n",
      "This is the link to this lab.\n",
      "The requests module allows you to send HTTP requests using Python.\n",
      "The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:\n",
      "Once you understand what is happening in the code above, it is fairly simple to pass this lab. Here's the solution to this lab:\n",
      "Let's move on to part 2 now where you'll build more on top of your existing code.\n",
      "This is the link to this lab.\n",
      "In this whole classroom, you’ll be using a library called BeautifulSoup in Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:\n",
      "Basically, BeautifulSoup can parse anything on the web you give it.\n",
      "Here’s a simple example of BeautifulSoup:\n",
      "Looking at the example above, you can see once we feed the page.content inside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:\n",
      "This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.\n",
      "This is the link to this lab.\n",
      "In the last lab, you saw how you can extract the title from the page. It is equally easy to extract out certain sections too. \n",
      "You also saw that you have to call .text on these to get the string, but you can print them without calling .text too, and it will give you the full markup. Try to run the example below:\n",
      "Let's take a look at how you can extract out body and head sections from your pages.\n",
      "When you try to print the page_body or page_head you'll see that those are printed as strings. But in reality, when you print(type page_body) you'll see it is not a string but it works fine.\n",
      "The solution of this example would be simple, based on the code above:\n",
      "This is the link to this lab.\n",
      "Now that you have explored some parts of BeautifulSoup, let's look how you can select DOM elements with BeautifulSoup methods.\n",
      "Once you have the soup variable (like previous labs), you can work with .select on it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let's look at an example:\n",
      ".select returns a Python list of all the elements. This is why you selected only the first element here with the [0] index.\n",
      "The solution for this lab is:\n",
      "Let's keep going.\n",
      "This is the link to this lab.\n",
      "Let's go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\n",
      "If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list called top_items. You will also extract out the reviews for these items as well.\n",
      "To pass this challenge, take care of the following things:\n",
      "There are quite a few tasks to be done in this challenge. Let's take a look at the solution first and understand what is happening:\n",
      "Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:\n",
      "Straightforward right?\n",
      "This is the link to this lab.\n",
      "So far you have seen how you can extract the text, or rather innerText of elements. Let's now see how you can extract attributes by extracting links from the page.\n",
      "Here’s an example of how to extract out all the image information from the page:\n",
      "In this lab, your task is to extract the href attribute of links with their text as well. Make sure of the following things:\n",
      "You are extracting the attribute values just like you extract values from a dict, using the get function. Let's take a look at the solution for this lab:\n",
      "Here, you extract the href attribute just like you did in the image case. The only thing you're doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.\n",
      "This is the link to this lab.\n",
      "Finally, let's understand how you can generate CSV from a set of data. You will create a CSV with the following headings:\n",
      "These products are located in the div.thumbnail. The CSV boilerplate is given below:\n",
      "You have to extract data from the website and generate this CSV for the three products.\n",
      "Let's see the solution to this lab:\n",
      "The for block is the most interesting here. You extract all the elements and attributes from what you've learned so far in all the labs. \n",
      "When you run this code, you end up with a nice CSV file. And that's about all the basics of web scraping with BeautifulSoup!\n",
      "I hope this interactive classroom from codedamn helped you understand the basics of web scraping with Python. \n",
      "If you liked this classroom and this blog, tell me about it on my twitter and Instagram. Would love to hear feedback!\n",
      "Independent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3\n",
      "\n",
      "    If you read this far, thank the author to show them you care. Say Thanks\n",
      "\n",
      "\n",
      "        Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. Get started\n",
      "\n",
      "freeCodeCamp is a donor-supported tax-exempt 501(c)(3) charity organization (United States Federal Tax Identification Number: 82-0779546)\n",
      "Our mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.\n",
      "Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.\n",
      "\n",
      "                You can make a tax-deductible donation here.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "paragraphs = soup.find_all('p')\n",
    "for para in paragraphs:\n",
    "    print(para.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a884fd0-2d54-4ca5-8fab-ddb31db47067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the webpage!\n",
      "Page Title: Web Scraping Python Tutorial – How to Scrape Data From A Website\n",
      "\n",
      "Python is a beautiful language to code in. It has a great package ecosystem, there's much less noise than you'll find in other languages, and it is super easy to use.\n",
      "Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping. \n",
      "In this article, we will cover how to use Python for web scraping. We'll also work through a complete hands-on classroom guide as we proceed.\n",
      "Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.\n",
      "If you want to code along, you can use this free codedamn classroom that consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.\n",
      "In this classroom, you'll be using this page to test web scraping: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\n",
      "This classroom consists of 7 labs, and you'll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.\n",
      "This is the link to this lab.\n",
      "The requests module allows you to send HTTP requests using Python.\n",
      "The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:\n",
      "Once you understand what is happening in the code above, it is fairly simple to pass this lab. Here's the solution to this lab:\n",
      "Let's move on to part 2 now where you'll build more on top of your existing code.\n",
      "This is the link to this lab.\n",
      "In this whole classroom, you’ll be using a library called BeautifulSoup in Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:\n",
      "Basically, BeautifulSoup can parse anything on the web you give it.\n",
      "Here’s a simple example of BeautifulSoup:\n",
      "Looking at the example above, you can see once we feed the page.content inside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:\n",
      "This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.\n",
      "This is the link to this lab.\n",
      "In the last lab, you saw how you can extract the title from the page. It is equally easy to extract out certain sections too. \n",
      "You also saw that you have to call .text on these to get the string, but you can print them without calling .text too, and it will give you the full markup. Try to run the example below:\n",
      "Let's take a look at how you can extract out body and head sections from your pages.\n",
      "When you try to print the page_body or page_head you'll see that those are printed as strings. But in reality, when you print(type page_body) you'll see it is not a string but it works fine.\n",
      "The solution of this example would be simple, based on the code above:\n",
      "This is the link to this lab.\n",
      "Now that you have explored some parts of BeautifulSoup, let's look how you can select DOM elements with BeautifulSoup methods.\n",
      "Once you have the soup variable (like previous labs), you can work with .select on it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let's look at an example:\n",
      ".select returns a Python list of all the elements. This is why you selected only the first element here with the [0] index.\n",
      "The solution for this lab is:\n",
      "Let's keep going.\n",
      "This is the link to this lab.\n",
      "Let's go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\n",
      "If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list called top_items. You will also extract out the reviews for these items as well.\n",
      "To pass this challenge, take care of the following things:\n",
      "There are quite a few tasks to be done in this challenge. Let's take a look at the solution first and understand what is happening:\n",
      "Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:\n",
      "Straightforward right?\n",
      "This is the link to this lab.\n",
      "So far you have seen how you can extract the text, or rather innerText of elements. Let's now see how you can extract attributes by extracting links from the page.\n",
      "Here’s an example of how to extract out all the image information from the page:\n",
      "In this lab, your task is to extract the href attribute of links with their text as well. Make sure of the following things:\n",
      "You are extracting the attribute values just like you extract values from a dict, using the get function. Let's take a look at the solution for this lab:\n",
      "Here, you extract the href attribute just like you did in the image case. The only thing you're doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.\n",
      "This is the link to this lab.\n",
      "Finally, let's understand how you can generate CSV from a set of data. You will create a CSV with the following headings:\n",
      "These products are located in the div.thumbnail. The CSV boilerplate is given below:\n",
      "You have to extract data from the website and generate this CSV for the three products.\n",
      "Let's see the solution to this lab:\n",
      "The for block is the most interesting here. You extract all the elements and attributes from what you've learned so far in all the labs. \n",
      "When you run this code, you end up with a nice CSV file. And that's about all the basics of web scraping with BeautifulSoup!\n",
      "I hope this interactive classroom from codedamn helped you understand the basics of web scraping with Python. \n",
      "If you liked this classroom and this blog, tell me about it on my twitter and Instagram. Would love to hear feedback!\n",
      "Independent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3\n",
      "\n",
      "    If you read this far, thank the author to show them you care. Say Thanks\n",
      "\n",
      "\n",
      "        Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. Get started\n",
      "\n",
      "freeCodeCamp is a donor-supported tax-exempt 501(c)(3) charity organization (United States Federal Tax Identification Number: 82-0779546)\n",
      "Our mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.\n",
      "Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.\n",
      "\n",
      "                You can make a tax-deductible donation here.\n",
      "            \n",
      "Image Source: https://cdn.freecodecamp.org/platform/universal/fcc_primary.svg\n",
      "Image Source: https://www.freecodecamp.org/news/content/images/size/w60/2021/05/mehul-mohan-gravatar.jpeg\n",
      "Image Source: https://www.freecodecamp.org/news/content/images/size/w2000/2020/09/webscrapingposter.jpg\n",
      "Image Source: https://www.freecodecamp.org/news/content/images/2020/09/screenzy-1601054558203.png\n",
      "Image Source: https://www.freecodecamp.org/news/content/images/size/w60/2021/05/mehul-mohan-gravatar.jpeg\n",
      "Image Source: https://cdn.freecodecamp.org/platform/universal/apple-store-badge.svg\n",
      "Image Source: https://cdn.freecodecamp.org/platform/universal/google-play-badge.svg\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/'\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the webpage!\")\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the page title\n",
    "    title = soup.title.text\n",
    "    print(f\"Page Title: {title}\")\n",
    "    \n",
    "    # Extract all paragraphs\n",
    "    paragraphs = soup.find_all('p')\n",
    "    for para in paragraphs:\n",
    "        print(para.text)\n",
    "    \n",
    "    # Optional: Extract all images\n",
    "    images = soup.find_all('img')\n",
    "    for img in images:\n",
    "        src = img.get('src')\n",
    "        print(f\"Image Source: {src}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7f0510c-3f2a-4046-a1b2-e423e9d92d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract article titles and publication dates\n",
    "articles = soup.find_all('article')  # Adjust this selector based on the website's HTML structure\n",
    "\n",
    "data = []\n",
    "for article in articles:\n",
    "    title = article.find('h2').text.strip()  # Adjust based on where the title is located\n",
    "    pub_date = article.find('time').text.strip()  # Adjust based on where the publication date is located\n",
    "    data.append({'Title': title, 'Publication Date': pub_date})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b7e299-5a67-4f58-948d-b72f8deea8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to articles.csv successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the extracted data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('articles.csv', index=False)\n",
    "\n",
    "print(\"Data has been written to articles.csv successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e622c83-8cda-44c4-9867-68aee7c9e2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Publication Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Introduction to Web Scraping classroom</td>\n",
       "      <td>September 25, 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title    Publication Date\n",
       "0  Introduction to Web Scraping classroom  September 25, 2020"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fb71d37-4536-4520-886a-70715a12d17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the webpage!\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the eBay listing\n",
    "url = 'https://www.ebay.com/itm/225313054444'\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the webpage!\")\n",
    "else:\n",
    "    print(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd444761-01db-437b-a3af-08b7baa9d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the item title\n",
    "title = soup.find('h1', class_='x-item-title__mainTitle').text.strip()\n",
    "\n",
    "# Extract the price\n",
    "price = soup.find('span', class_='x-price-approx__price').text.strip() if soup.find('span', class_='x-price-approx__price') else 'N/A'\n",
    "\n",
    "# Extract the condition\n",
    "condition = soup.find('div', class_='d-item-condition').text.strip() if soup.find('div', class_='d-item-condition') else 'N/A'\n",
    "\n",
    "# Create a dictionary with the extracted data\n",
    "data = {\n",
    "    'Title': title,\n",
    "    'Price': price,\n",
    "    'Condition': condition\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dab08ae7-f374-4059-9654-17fee38c5d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to ebay_item.csv successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the extracted data\n",
    "df = pd.DataFrame([data])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('ebay_item.csv', index=False)\n",
    "\n",
    "print(\"Data has been written to ebay_item.csv successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f74c7eb-79e1-48e1-b3af-ad33c0d3de4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the webpage!\n",
      "Data has been written to ebay_item.csv successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the eBay listing\n",
    "url = 'https://www.ebay.com/itm/225313054444'\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the webpage!\")\n",
    "else:\n",
    "    print(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the item title\n",
    "title = soup.find('h1', class_='x-item-title__mainTitle').text.strip()\n",
    "\n",
    "# Extract the price\n",
    "price = soup.find('span', class_='x-price-approx__price').text.strip() if soup.find('span', class_='x-price-approx__price') else 'N/A'\n",
    "\n",
    "# Extract the condition\n",
    "condition = soup.find('div', class_='d-item-condition').text.strip() if soup.find('div', class_='d-item-condition') else 'N/A'\n",
    "\n",
    "# Create a dictionary with the extracted data\n",
    "data = {\n",
    "    'Title': title,\n",
    "    'Price': price,\n",
    "    'Condition': condition\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df = pd.DataFrame([data])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('ebay_item.csv', index=False)\n",
    "\n",
    "print(\"Data has been written to ebay_item.csv successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "123ec775-a0c9-405f-be84-84b3eb43e054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.6\" TOUCH Dell laptop Precision 5530 i7 8850...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Price  Condition\n",
       "0  15.6\" TOUCH Dell laptop Precision 5530 i7 8850...    NaN        NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"ebay_item.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6379d-4085-45f6-a0f2-b8d555561276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
